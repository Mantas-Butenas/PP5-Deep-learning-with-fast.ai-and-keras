#1
learn.fit_one_cycle(5)

epoch     train_loss  valid_loss  _rmse     time
0         652.610046  590.232666  24.294704 00:00
1         637.812256  573.488220  23.947617 00:00
2         619.957092  555.477966  23.568579 00:00
3         608.414429  547.531433  23.399389 00:00
4         599.960327  541.656555  23.273516 00:00
--------------------------------------------------------
#2 Much better results with increased number of epochs and learning rate
learn.fit_one_cycle(10, lr_max=1e-2(0.01))
learn.summary()
epoch     train_loss  valid_loss  _rmse     time
0         623.742126  650.100708  25.497074 00:00
1         595.961365  593.482056  24.361486 00:00
2         559.284119  501.601349  22.396458 00:00
3         513.341675  355.998779  18.867928 00:00
4         449.761993  211.608093  14.546755 00:00
5         373.368469  92.066383   9.595122  00:00
6         302.501770  32.275787   5.681178  00:00
7         242.778748  16.343763   4.042742  00:00
8         195.918991  11.695019   3.419798  00:00
9         159.237747  10.996536   3.316103  00:00
-------------------------------------------------------
#3 Even better results with bigger number of epochs
learn.fit_one_cycle(20, lr_max=1e-2)
learn.summary()
epoch     train_loss  valid_loss  _rmse     time
0         621.383057  657.896057  25.649485 00:00
1         604.908875  617.161499  24.842735 00:00
2         581.942139  557.614258  23.613861 00:00
3         556.874390  490.977905  22.158022 00:00
4         520.208008  334.333679  18.284794 00:00
5         460.458679  190.446716  13.800244 00:00
6         377.223938  30.113342   5.487563  00:00
7         298.446991  6.879346    2.622851  00:00
8         238.533554  7.785973    2.790336  00:00
9         192.628082  2.728956    1.651955  00:00
10        156.798874  3.674139    1.916804  00:00
11        128.411224  3.298053    1.816054  00:00
12        105.644470  2.261315    1.503767  00:00
13        87.200638   2.637387    1.624003  00:00
14        72.506752   2.052046    1.432497  00:00
15        60.261898   2.231709    1.493890  00:00
16        50.540676   2.082286    1.443013  00:00
17        42.384827   1.897096    1.377351  00:00
18        35.816196   1.907323    1.381058  00:00
19        30.407433   1.871455    1.368011  00:00
--------------------------------------------------------
#4 It looks like we won't get improvement with a bigger number of epochs with this learning rate, after 27th epoch RMSE increased.
learn.fit_one_cycle(30, lr_max=1e-2)
learn.summary()

epoch     train_loss  valid_loss  _rmse     time
0         623.312744  656.190247  25.616209 00:00
1         608.185181  625.799316  25.015982 00:00
2         591.258545  576.273254  24.005692 00:00
3         576.787292  539.464417  23.226374 00:00
4         552.974731  492.971558  22.202963 00:00
5         522.629517  342.902374  18.517624 00:00
6         469.366028  185.805649  13.631055 00:00
7         392.023315  33.330982   5.773299  00:00
8         314.164948  9.697202    3.114033  00:00
9         253.756714  8.011702    2.830495  00:00
10        206.384537  3.653697    1.911465  00:00
11        168.881531  4.153416    2.037993  00:00
12        139.070114  5.523215    2.350152  00:00
13        115.284805  4.084856    2.021103  00:00
14        95.786026   3.320837    1.822316  00:00
15        79.597076   2.219959    1.489953  00:00
16        66.356697   3.493451    1.869078  00:00
17        55.638729   2.403664    1.550375  00:00
18        46.669735   2.276143    1.508689  00:00
19        39.273766   2.355581    1.534790  00:00
20        33.326565   2.019553    1.421110  00:00
21        28.365379   1.969155    1.403266  00:00
22        24.195606   1.795724    1.340046  00:00
23        20.567774   1.896684    1.377202  00:00
24        17.660732   1.893149    1.375917  00:00
25        15.073755   1.861375    1.364322  00:00
26        13.090066   1.837576    1.355572  00:00
27        11.443614   1.825562    1.351134  00:00
28        9.983084    1.855552    1.362186  00:00
29        8.867616    1.874750    1.369215  00:00
----------------------------------------------------------------------
#5 Decreased learning rate, seems a little bit too slow of an improvement.

learn.fit_one_cycle(30, lr_max=1e-3)

epoch     train_loss  valid_loss  _rmse     time
0         636.872314  682.087036  26.116796 00:00
1         633.685852  675.517883  25.990726 00:00
2         626.098206  656.856812  25.629219 00:00
3         613.996704  629.612976  25.092089 00:00
4         603.675232  606.685547  24.630987 00:00
5         594.462830  586.128540  24.210091 00:00
6         586.103149  573.747864  23.953033 00:00
7         578.129028  556.293884  23.585884 00:00
8         570.684448  549.268250  23.436472 00:00
9         563.454895  544.338684  23.331068 00:00
10        554.101990  531.227295  23.048370 00:00
11        545.529785  526.137451  22.937685 00:00
12        537.924866  516.788818  22.732992 00:00
13        531.010376  516.138672  22.718685 00:00
14        523.123718  498.593964  22.329218 00:00
15        516.624878  492.776031  22.198559 00:00
16        509.101715  490.323364  22.143246 00:00
17        500.760468  468.877411  21.653578 00:00
18        494.846588  469.341309  21.664288 00:00
19        487.629944  457.123627  21.380449 00:00
20        482.316711  455.299438  21.337748 00:00
21        475.583588  442.604767  21.038174 00:00
22        468.558167  438.513184  20.940706 00:00
23        462.954285  436.760254  20.898809 00:00
24        457.899536  435.710938  20.873690 00:00
25        453.778595  434.060760  20.834124 00:00
26        449.379456  431.213135  20.765671 00:00
27        445.829956  429.819153  20.732079 00:00
28        444.376556  436.292877  20.887625 00:00
29        442.772339  435.597748  20.870979 00:00
----------------------------------------------------------------------
# 6 learning rate changed to be in the middle of 1e-3 and 1e-2, seemed to be an improvement.

learn.fit_one_cycle(30, lr_max=5e-3)

epoch     train_loss  valid_loss  _rmse     time
0         644.637024  626.217163  25.024334 00:00
1         631.424377  610.318848  24.704634 00:00
2         616.553528  583.217285  24.149891 00:00
3         604.432007  564.072388  23.750210 00:00
4         587.459412  543.759521  23.318651 00:00
5         571.430420  525.955017  22.933708 00:00
6         549.414490  457.289307  21.384323 00:00
7         521.453552  378.867737  19.464525 00:00
8         481.954620  285.701263  16.902700 00:00
9         432.852417  189.555481  13.767917 00:00
10        376.779205  85.462990   9.244619  00:00
11        318.883881  35.516514   5.959573  00:00
12        265.561005  13.353932   3.654303  00:00
13        220.048080  4.463730    2.112754  00:00
14        181.816330  2.836203    1.684103  00:00
15        151.075989  3.484819    1.866767  00:00
16        125.920647  2.645994    1.626651  00:00
17        105.035271  2.290863    1.513559  00:00
18        87.618706   2.700877    1.643435  00:00
19        73.156815   2.187831    1.479132  00:00
20        61.245964   2.153787    1.467579  00:00
21        51.410576   2.007378    1.416820  00:00
22        43.383793   2.214010    1.487955  00:00
23        36.796066   2.216019    1.488630  00:00
24        31.104908   1.998292    1.413610  00:00
25        26.406301   1.918227    1.385001  00:00
26        22.631823   1.868025    1.366757  00:00
27        19.251419   1.942309    1.393667  00:00
28        16.608387   1.918529    1.385110  00:00
29        14.303354   1.887607    1.373902  00:00
----------------------------------------------------------------------
# 7 Got our lowest RMSE score, but the model stopped improving after 39th epoch, so there is no need to increase the epoch further

learn.fit_one_cycle(40, lr_max=5e-3)

epoch     train_loss  valid_loss  _rmse     time
0         643.788208  639.496399  25.288263 00:00
1         628.180969  628.256409  25.065044 00:00
2         617.382874  607.225159  24.641939 00:00
3         607.502258  585.458679  24.196253 00:00
4         595.540283  570.508789  23.885324 00:00
5         581.743103  556.604553  23.592468 00:00
6         565.437988  517.770813  22.754580 00:00
7         545.441650  454.812134  21.326324 00:00
8         521.010925  377.049652  19.417767 00:00
9         485.895111  277.161499  16.648169 00:00
10        438.721771  183.296997  13.538723 00:00
11        383.208160  86.325867   9.291172  00:00
12        324.581055  37.592873   6.131303  00:00
13        270.543182  12.983955   3.603326  00:00
14        223.759277  4.331517    2.081230  00:00
15        185.292038  3.426623    1.851114  00:00
16        154.135895  4.623999    2.150349  00:00
17        128.368317  3.865732    1.966146  00:00
18        107.227905  2.654568    1.629285  00:00
19        89.489571   1.814823    1.347154  00:00
20        74.917824   1.842023    1.357212  00:00
21        62.868977   1.756165    1.325204  00:00
22        52.791248   1.867921    1.366719  00:00
23        44.604366   1.703851    1.305316  00:00
24        37.601124   2.097872    1.448403  00:00
25        31.733145   1.743871    1.320557  00:00
26        26.922832   2.069841    1.438694  00:00
27        22.930698   1.623036    1.273984  00:00
28        19.989933   1.571207    1.253478  00:00
29        17.269651   1.734407    1.316969  00:00
30        14.831453   1.693499    1.301345  00:00
31        12.774866   1.637766    1.279752  00:00
32        11.189390   1.666047    1.290754  00:00
33        9.938547    1.689764    1.299909  00:00
34        8.774916    1.690395    1.300152  00:00
35        7.797193    1.673720    1.293723  00:00
36        6.911728    1.651335    1.285043  00:00
37        6.277825    1.642804    1.281719  00:00
38        5.707047    1.588188    1.260233  00:00
39        5.389653    1.599458    1.264697  00:00

Final metrics: learn.fit_one_cycle(38, lr_max=5e-3)